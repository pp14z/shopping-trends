# Shopping Trends - ETL, API REST y Dashboard de An√°lisis

## üìå Descripci√≥n general

Soluci√≥n end-to-end para el an√°lisis de tendencias de compra. El sistema extrae datos de transacciones (3,900+ registros), los transforma con Python/Pandas, los almacena en PostgreSQL y los expone a trav√©s de una API REST para su visualizaci√≥n en un dashboard interactivo.

**Tecnolog√≠as**:
- **ETL**: Pandas, Celery
- **Backend**: Django REST Framework (DRF), PostgreSQL
- **Frontend**: Next.js, TypeScript, Shadcn/ui
- **Infraestructura**: Docker-compose

---

## üöÄ Funcionalidades Clave

### ETL
- Ingesta de datos simulando una carga desde S3 (archivo CSV local).
- Limpieza y transformaci√≥n de datos con Pandas.
- Normalizaci√≥n del dataset en un modelo relacional (clientes, productos, transacciones).
- Carga optimizada a PostgreSQL utilizando inserciones masivas (`bulk_create`).
- Estructura preparada para ejecuci√≥n peri√≥dica con Celery (actualizaciones autom√°ticas).

### API REST
- Endpoints organizados por recurso: clientes, productos, ventas.
- Soporte para filtros y b√∫squedas por par√°metros clave.
- Documentaci√≥n generada autom√°ticamente con Swagger / OpenAPI.
- Validaci√≥n de datos y manejo de errores consistente.

### Dashboard Interactivo
- Visualizaci√≥n de KPIs: ventas totales, productos m√°s vendidos, ingresos promedio por categor√≠a.
- Gr√°ficos din√°micos integrados con Chart.js o Recharts.
- Filtros interactivos por categor√≠a, temporada y m√©todo de pago.
- Interfaz moderna desarrollada en Next.js con TypeScript y Shadcn/ui.

### Infraestructura
- Proyecto dockerizado: backend, frontend y base de datos.
- Scripts reproducibles para carga de datos y entorno local.
- Preparado para integraci√≥n continua y despliegue automatizado.

---

## ‚úÖ TODO

### **Setup Inicial**
- [X] Estructura de proyecto (backend/frontend/data)
- [X] Setup de Django y DRF
- [X] Configurar Docker-compose
- [X] Crear modelos base

### **ETL**
- [X] Crear script para limpiar y validar data
- [X] Crear script para cargar data
- [X] Optimizar carga con `bulk_create`
- [X] Crear task para ejecutar el proceso ETL de forma programada
- [X] Hacer pruebas de `clean_data` y `load_data`
- [X] A√±adir management command para correr ETL manualmente
- [?] Simular actualizaciones peri√≥dicas con Celery

### **API**
- [X] Crear endpoint `/api/customers/insights/` para ver Customer Insights
- [X] Implementar filtros din√°micos
- [X] A√±adir anotaciones y agregaciones a los insights
- [ ] Hacer pruebas del endpoint con distintos filtros y combinaciones
- [ ] Cachear resultados e invalidarlos cuando se cargue nueva data

### **Frontend**
- [ ] ...

### **Deployment**
- [ ] ...

---

## Decisiones T√©cnicas

### üóÑÔ∏è Modelado de la Base de Datos

Se normaliz√≥ la informaci√≥n en cuatro tablas principales: Clientes, Productos, Variantes de Productos y √ìrdenes. Se definieron √≠ndices en los campos que se anticipan como los m√°s utilizados para filtrar la informaci√≥n en el panel de control. Se establecieron valores predeterminados (`defaults`) para campos donde la ausencia de datos no afectar√≠a significativamente el an√°lisis estad√≠stico. Tambi√©n se identificaron los campos obligatorios, cuya ausencia resultar√≠a en la exclusi√≥n del registro para mantener la integridad de los datos esenciales.

### üßº Limpieza y Validaci√≥n de Datos (ETL)

Se dise√±√≥ una funci√≥n de limpieza (`clean_data`) que valida la estructura del archivo CSV, descarta filas incompletas o duplicadas y aplica reglas de normalizaci√≥n para asegurar la coherencia de los datos antes de cargarlos en la base de datos. Se descartaron registros sin campos cr√≠ticos como ID del cliente, producto o monto de compra, y se eliminaron duplicados basados en `customer id`, que se asume como identificador √∫nico.

Se definieron valores por defecto basados en los modelos (`choices`) para los campos faltantes, como `"unknown"` para valores categ√≥ricos, `False` para booleanos, y mediana o moda para datos num√©ricos. Adem√°s, se estandariz√≥ todo el texto a min√∫sculas y sin espacios extra, y se garantiz√≥ que cada producto tuviera una √∫nica categor√≠a consistente, basada en la moda por nombre de producto.

### üì• Carga de Datos en Base de Datos

Se implement√≥ la funci√≥n `load_data` para poblar las tablas del sistema utilizando el ORM de Django de forma eficiente y segura. Se emple√≥ `bulk_create` para minimizar la cantidad de queries al insertar nuevos registros de clientes, productos, variantes y √≥rdenes.

Para evitar errores por duplicados en clientes, se consultaron previamente los `customer_id` existentes en la base de datos y se excluyeron del conjunto a insertar. En lugar de utilizar `get_or_create` dentro de bucles ‚Äîlo cual implicar√≠a una gran cantidad de queries‚Äî, se construyeron diccionarios de b√∫squeda (`product_lookup`, `variant_lookup`, etc.) que mapean combinaciones clave como `(name, category, season)` para productos y `(product, size, color)` para variantes, permitiendo resolver referencias sin acceso repetido a la base de datos.

Una vez resueltas todas las relaciones con `Customer` y `ProductVariant`, se procedi√≥ a insertar las √≥rdenes tambi√©n mediante `bulk_create`. Este enfoque asegura una carga de datos robusta, eficiente y libre de errores de integridad, optimizada para grandes vol√∫menes de informaci√≥n.

---

## Futuras mejoras
- Agrupar colores de productos por sombras/color principal?

---
